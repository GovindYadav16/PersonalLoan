================================================================================
ALGORITHMS AND PARAMETERS USED IN train.py - DETAILED EXPLANATION
================================================================================

This document explains the algorithms and parameters currently implemented
in your train.py file for the Personal Loan prediction project.

================================================================================
OVERVIEW: TWO ALGORITHMS IMPLEMENTED
================================================================================

Your train.py currently supports:
1. Random Forest Classifier (Lines 22-45)
2. Logistic Regression (Lines 48-63)

Both algorithms are called from the main train_model() function (Lines 95-150)
based on the configuration in config.yaml.

================================================================================
1. RANDOM FOREST CLASSIFIER
================================================================================

Location in Code: train_random_forest() function (Lines 22-45)

Algorithm Description:
----------------------
Random Forest is an ensemble learning method that combines multiple decision
trees. Each tree is trained on a random subset of the data (bootstrap sample)
and considers a random subset of features when making splits. The final
prediction is made by taking the majority vote from all trees.

How It Works:
1. Creates many decision trees (n_estimators parameter)
2. Each tree sees a random subset of training data
3. Each tree uses a random subset of features for splitting
4. All trees vote, and the majority class wins
5. This reduces overfitting compared to a single tree

Why Use It for Personal Loan Prediction:
- Handles non-linear relationships between customer features and loan acceptance
- Robust to outliers (e.g., unusual income or age values)
- Provides feature importance (which factors matter most for loan approval)
- Less likely to overfit than a single decision tree
- Works well with mixed data types (numerical and categorical features)

PARAMETERS USED IN YOUR CODE:
==============================

1. n_estimators: int = 100
   - Default Value: 100
   - What It Does: Number of decision trees in the forest
   
   Significance:
   * More trees = Better performance (up to a point), but slower training
   * Fewer trees = Faster training, but may underperform
   * 100 is a good default - provides good balance
   * Typical range: 50-500 trees
   * After ~200 trees, improvement often plateaus
   
   Example Impact:
   - n_estimators=10: Fast but may miss patterns (underfitting)
   - n_estimators=100: Good balance (YOUR DEFAULT)
   - n_estimators=500: More accurate but takes 5x longer to train
   
   In Your Config (config.yaml):
   - Can be set via: model.n_estimators (defaults to 100)

---

2. max_depth: int = None
   - Default Value: None (unlimited depth)
   - What It Does: Maximum depth each tree can grow to
   
   Significance:
   * None = Trees grow until leaves are pure (all same class) or cannot split further
   * Shallow trees (depth=3-10) = Simpler model, prevents overfitting
   * Deep trees (depth=None or high) = Captures complex patterns, risk of overfitting
   
   Example Impact:
   - max_depth=3: Very simple trees, easy to interpret, may underfit
   - max_depth=10: Moderate complexity, good generalization
   - max_depth=None: Trees grow fully, captures all patterns, may overfit
   
   Why None is Often OK for Random Forest:
   * Random Forest uses many trees - even if individual trees overfit,
     the ensemble (voting) reduces overfitting
   * Bootstrap sampling (random data subsets) provides natural regularization
   * Random feature selection adds diversity
   
   When to Limit max_depth:
   - Training is too slow (limit to 10-20)
   - Need more interpretable trees (limit to 5-10)
   - Model is overfitting (check validation metrics)
   
   In Your Config (config.yaml):
   - Can be set via: model.max_depth (defaults to None)
   - Note: In YAML, use "null" or omit the key

---

3. random_state: int = 42
   - Default Value: 42
   - What It Does: Seed for random number generator
   
   Significance:
   * Ensures reproducibility - same data + same seed = same model
   * Randomness comes from:
     - Bootstrap sampling (random data subsets per tree)
     - Random feature selection (random features per split)
   * Without random_state: Different runs give different models
   * With random_state: Always get same results
   
   Why 42?
   - Common convention in data science ("Answer to Life, Universe, Everything")
   - Can use any integer: 1, 42, 123, 2024, etc.
   - Same seed across experiments ensures fair comparison
   
   Example Impact:
   - random_state=42: Always reproducible results
   - random_state=None: Different results each run (harder to debug)
   
   In Your Config (config.yaml):
   - Can be set via: model.random_state (defaults to 42)

---

4. n_jobs: int = -1
   - What It Does: Number of CPU cores to use for parallel processing
   - Value in Code: -1 (uses ALL available CPU cores)
   
   Significance:
   * Random Forest can train trees in parallel (they're independent)
   * n_jobs=-1: Use all CPU cores (fastest training)
   * n_jobs=1: Use single core (slower but uses less memory)
   * n_jobs=4: Use 4 cores (if you have 8+ cores, good middle ground)
   
   Performance Impact:
   - 4 cores: ~4x faster than 1 core
   - 8 cores: ~6-7x faster (diminishing returns due to overhead)
   
   When to Change:
   - If other processes need CPU: Use n_jobs=1 or 2
   - If running out of memory: Use fewer cores (less parallelism)

---

PARAMETERS NOT EXPLICITLY SET (Using sklearn defaults):
=========================================================

These parameters use sklearn's default values:

1. min_samples_split: int = 2
   - Minimum samples needed to split a node
   - 2 = Split if at least 2 samples
   - Increasing this prevents overfitting

2. min_samples_leaf: int = 1
   - Minimum samples required in a leaf node
   - 1 = Leaf can have just 1 sample
   - Increasing this smooths predictions

3. max_features: 'sqrt' (default)
   - Number of features to consider per split
   - 'sqrt' = sqrt(total_features) features randomly chosen
   - This is why it's "random" forest - random feature selection!

4. criterion: 'gini' (default)
   - How to measure split quality
   - 'gini': Gini impurity (fast, works well)
   - 'entropy': Information gain (slower, similar results)

5. bootstrap: True (default)
   - Whether to use bootstrap sampling
   - True = Each tree sees random subset of data (recommended)

6. class_weight: None (default)
   - How to handle imbalanced classes
   - None = Equal weights for both classes
   - 'balanced' = Automatically adjust weights for imbalanced data

================================================================================
2. LOGISTIC REGRESSION
================================================================================

Location in Code: train_logistic_regression() function (Lines 48-63)

Algorithm Description:
----------------------
Logistic Regression is a linear classification algorithm that uses the
logistic (sigmoid) function to model the probability of a binary outcome.
It finds the best linear combination of features to predict class probabilities.

How It Works:
1. Calculates linear combination: z = w1*x1 + w2*x2 + ... + wn*xn + b
2. Applies sigmoid function: P(class=1) = 1 / (1 + e^(-z))
3. Outputs probability between 0 and 1
4. If probability > 0.5: Predict class 1, else predict class 0

Why Use It for Personal Loan Prediction:
- Highly interpretable - coefficients show feature importance
- Fast training and prediction
- Provides probability scores (not just 0/1 predictions)
- Good baseline model for comparison
- Works well when relationships are somewhat linear
- Useful for stakeholders who need to understand "why"

PARAMETERS USED IN YOUR CODE:
==============================

1. random_state: int = 42
   - Default Value: 42
   - What It Does: Seed for random number generator
   
   Significance:
   * Some solvers use randomness for initialization
   * Ensures reproducible results across runs
   * Same meaning as in Random Forest
   
   In Your Config (config.yaml):
   - Can be set via: model.random_state (defaults to 42)

---

2. max_iter: int = 1000
   - Default Value: 1000
   - What It Does: Maximum number of iterations for the solver to converge
   
   Significance:
   * Logistic Regression uses iterative optimization to find best coefficients
   * max_iter limits how many iterations the solver tries
   * If convergence warning appears: Increase max_iter
   * Default in sklearn is 100, but you set 1000 (more generous)
   
   Why 1000?
   - Large or complex datasets may need more iterations
   - 1000 is safe default - prevents convergence warnings
   - Usually converges in 100-300 iterations
   
   When to Change:
   - If you see: "ConvergenceWarning: max_iter reached"
     → Increase max_iter to 2000 or 5000
   - If training is too slow: Try 500 first
   
   Example Impact:
   - max_iter=100: May not converge on complex data (sklearn default)
   - max_iter=1000: Your setting - should work for most cases
   - max_iter=5000: Very safe, but unnecessary for most datasets

---

PARAMETERS NOT EXPLICITLY SET (Using sklearn defaults):
=========================================================

These parameters use sklearn's default values:

1. penalty: 'l2' (default)
   - Type of regularization
   - 'l2': Ridge regularization (shrinks coefficients toward 0)
   - Prevents overfitting by penalizing large coefficients
   - You could use 'l1' for feature selection (sparse model)

2. C: 1.0 (default)
   - Inverse of regularization strength
   - Higher C = Less regularization = Fit training data more closely
   - Lower C = More regularization = More generalization
   - Typical range: 0.001 to 100
   - C=1.0 is a good middle ground

3. solver: 'lbfgs' (default)
   - Optimization algorithm
   - 'lbfgs': Good for small datasets, supports l2 penalty
   - For large datasets, 'saga' or 'sag' might be faster

4. class_weight: None (default)
   - How to handle imbalanced classes
   - None = Equal weights
   - 'balanced' = Automatically adjust for imbalanced data
   - For loan prediction, if loan acceptances are rare, use 'balanced'

5. fit_intercept: True (default)
   - Whether to include intercept (bias) term
   - True = Model includes intercept (usually better)

================================================================================
HOW THE ALGORITHMS ARE CALLED IN train_model()
================================================================================

Location: Lines 129-143 in train_model() function

Code Flow:
----------
1. Load configuration from config.yaml
2. Check model.type: 'random_forest' or 'logistic_regression'
3. Call appropriate training function with parameters from config

Example Configuration (config.yaml):
------------------------------------
model:
  type: "random_forest"  # or "logistic_regression"
  n_estimators: 100      # Only used for Random Forest
  max_depth: null        # Only used for Random Forest
  random_state: 42       # Used by both algorithms

How Parameters Are Passed:
--------------------------
- config.get('n_estimators', 100): Get from config, or default to 100
- config.get('max_depth', None): Get from config, or default to None
- config.get('random_state', 42): Get from config, or default to 42

This means:
- You can set parameters in config.yaml
- If not set, sensible defaults are used
- Easy to experiment with different parameter values

================================================================================
COMPARISON: RANDOM FOREST vs LOGISTIC REGRESSION
================================================================================

For Personal Loan Prediction:

RANDOM FOREST:
-------------
✅ Pros:
   - Usually higher accuracy (captures non-linear patterns)
   - Handles feature interactions automatically
   - Feature importance available
   - Robust to outliers and missing values
   - No need for feature scaling

❌ Cons:
   - Less interpretable (black box)
   - Slower to train (but still fast with n_jobs=-1)
   - More memory usage

Best When:
- You want highest accuracy
- Non-linear relationships exist (income^2 affects loan approval)
- Need feature importance rankings

LOGISTIC REGRESSION:
--------------------
✅ Pros:
   - Highly interpretable (coefficient = feature impact)
   - Very fast training and prediction
   - Provides probabilities
   - Low memory usage
   - Good baseline model

❌ Cons:
   - Assumes linear relationships
   - Lower accuracy if data is non-linear
   - Sensitive to outliers
   - May need feature engineering

Best When:
- Interpretability is critical (explain to stakeholders)
- Linear relationships expected
- Need fast predictions
- Baseline for comparison

RECOMMENDATION FOR YOUR PROJECT:
--------------------------------
1. Start with Logistic Regression - quick baseline, interpretable
2. Then try Random Forest - likely better accuracy
3. Compare results - does Random Forest improve significantly?
4. If yes and interpretability not critical → Use Random Forest
5. If interpretability critical → Use Logistic Regression

================================================================================
COMMON PARAMETER TUNING SCENARIOS
================================================================================

SCENARIO 1: Model is Underfitting (Low Accuracy on Training Data)
-------------------------------------------------------------------
Random Forest:
- Increase n_estimators: 100 → 200 or 300
- Increase max_depth: None → 15 or 20
- Decrease min_samples_split: 2 → 1

Logistic Regression:
- Increase max_iter: 1000 → 2000 or 5000
- Increase C: 1.0 → 10.0 or 100.0 (less regularization)

SCENARIO 2: Model is Overfitting (High Training, Low Test Accuracy)
--------------------------------------------------------------------
Random Forest:
- Decrease n_estimators: 100 → 50 (fewer trees)
- Decrease max_depth: None → 5 or 10 (shallower trees)
- Increase min_samples_split: 2 → 10 or 20
- Increase min_samples_leaf: 1 → 5

Logistic Regression:
- Decrease C: 1.0 → 0.1 or 0.01 (more regularization)
- Change penalty: 'l2' → 'l1' (Lasso - feature selection)

SCENARIO 3: Imbalanced Classes (Few Loan Acceptances)
------------------------------------------------------
Both Algorithms:
- Add class_weight='balanced' parameter
  Example for Random Forest:
    RandomForestClassifier(..., class_weight='balanced')
  
  Example for Logistic Regression:
    LogisticRegression(..., class_weight='balanced')

SCENARIO 4: Training is Too Slow
---------------------------------
Random Forest:
- Decrease n_estimators: 100 → 50
- Set max_depth: None → 10 or 15
- Use n_jobs=2 instead of -1 (fewer cores)

Logistic Regression:
- Decrease max_iter: 1000 → 500
- Change solver: 'lbfgs' → 'liblinear' (faster for small data)

SCENARIO 5: Need Better Accuracy
---------------------------------
Random Forest:
- Increase n_estimators: 100 → 200 or 300
- Keep max_depth=None (let trees grow)
- Tune max_features: 'sqrt' → 'log2' or try different values

Logistic Regression:
- Try feature engineering (polynomial features)
- Consider switching to Random Forest (better for non-linear)

================================================================================
QUICK REFERENCE: PARAMETER MEANINGS
================================================================================

Random Forest Parameters:
-------------------------
n_estimators    → More trees = Better (up to a point), slower
max_depth       → Deeper = More complex, risk overfitting
random_state    → Ensures reproducibility (same seed = same results)
n_jobs          → More cores = Faster training (-1 = all cores)

Logistic Regression Parameters:
-------------------------------
random_state    → Ensures reproducibility
max_iter        → More iterations = Better convergence, slower
C               → Higher = Less regularization, more fitting (default: 1.0)
penalty         → 'l2' = Ridge (default), 'l1' = Lasso (sparse)

Both Algorithms:
----------------
class_weight    → 'balanced' = Handle imbalanced classes automatically

================================================================================
RECOMMENDED NEXT STEPS FOR YOUR PROJECT
================================================================================

1. Current State:
   - You have Random Forest (default) and Logistic Regression implemented
   - Good default parameters set

2. To Improve Model:
   - Check class imbalance: Use class_weight='balanced' if needed
   - Tune n_estimators: Try 50, 100, 200, 300 and compare
   - Tune max_depth: Try 5, 10, 15, None and compare
   - Use cross-validation for proper evaluation

3. To Add More Algorithms:
   - XGBoost (often best accuracy)
   - Gradient Boosting (similar to Random Forest)
   - SVM (for comparison)

4. To Better Understand Results:
   - Plot feature importance (Random Forest)
   - Analyze coefficients (Logistic Regression)
   - Compare both models on same test set

================================================================================
END OF DOCUMENT
================================================================================

For more details on all algorithms, see: binary_classification_algorithms.txt
